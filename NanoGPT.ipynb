{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5vDxp93SoD4gvqllhZUUn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/egioia92/NanoGPT/blob/main/NanoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The base components of GPT comes from a paper called \"Attetion is all you need\" that proposed the Transormer architecture. GPT stands for Generatively Pretrained Transformer, so transformer is the neural network that does all the heavy work under the wood.\n",
        "\n",
        "The transformer architecture has been copy pasted with small modification in a huge number of application in AI and that includes at the core chatgpt.\n",
        "\n",
        "Let's build a transformer based language model in particular a character level language model trained on all Shakespare\n"
      ],
      "metadata": {
        "id": "887iBcryyOGQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5KbAA_FyMED",
        "outputId": "228ed575-da03-4488-f7cb-ef29e50d252e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-07 15:39:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-04-07 15:39:18 (35.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#We always start with a dataset to train on\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We read it in to inspect it\n",
        "with open ('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "8Ahvlolv1x8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters:\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeJ55ufK2XOX",
        "outputId": "e2587d03-baa2-4f4d-fcd5-217c0b8c5dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3-Tznxn21MA",
        "outputId": "5cd38d8b-4a40-4ec1-ba89-61a7c21ab343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's see all the unique characters that occur in the text\n",
        "#Sets are an unordered collection of unique elements\n",
        "#These are the possible characters that the model can see or emit\n",
        "chars = sorted(set(text))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWEkxDH82-XM",
        "outputId": "6d6d31a7-1cee-446b-e97c-ca09ef8da837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We want to develop some strategy to tokenize the input text\n",
        "#Tokenise means to convert the row text to some integers according to a vocabulary of possible elements\n",
        "#In our case it means to create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } #iterate over the character and create a lookup table (noting to do with stoi c function)\n",
        "itos = { i:ch for i,ch in enumerate(chars) } #iterate over the character and create a lookup table\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]          # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) #decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"ciao belli\"))\n",
        "print(decode(encode(\"ciao belli\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTi3343Y5n1O",
        "outputId": "07434b0d-1386-4658-8024-a2075baf67be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[41, 47, 39, 53, 1, 40, 43, 50, 50, 47]\n",
            "ciao belli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "#A torch.Tensor is a multi-dimensional matrix containing elements of a single data type\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long) #A torch.dtype is an object that represents the data type of a torch.Tensor\n",
        "\n",
        "print(data.shape,data.dtype)\n",
        "print(data[:1000]) #the 1000 character we looked at earlier will to the GPT look like this\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6bEtbhD5wem",
        "outputId": "e119a3c3-9891-4eea-f89b-bff839807e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's now split uo the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "QVHrR0HMMl8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We define a block size: the chunk of data that we randomly sample from the\n",
        "#Train set to train the transformer\n",
        "block_size = 8\n",
        "train_data[:block_size+1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwuUCZpONY7W",
        "outputId": "4e2e7034-baea-4b2e-f946-742a24530f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size] # x are the input to the transformer. The first block_size characters\n",
        "y = train_data[1:block_size+1] # y will the next block_size characters. It is offset by one because y is the target for each position in the input x\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd0AIVWWOb7_",
        "outputId": "2dcb0edd-88eb-4221-83ff-15235c1932c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We define the batch: a group of many samble of size equal to block size\n",
        "# stack in a single tensor. This is done for efficiencty to keeo the gpu busy\n",
        "# and let it work on different samples of block size in parallel\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4 #how many indipendent sequences will we process in parallel (every forward/backward pass in the transformer)?\n",
        "block_size = 8 #what is the maximum context lenght to make a prediction?\n",
        "\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  # Generating random positions from which take the chanck of data off\n",
        "  #We generate batch_size numbers of random offset\n",
        "  # In this case ix is gonna be 4 numbers that are randomly generated between zero and (len(data) - block_size)\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  #torch stack takes the random chosen 1-dimentional tensor and stack them togheter as rows of a 4 by 8 dimention tensor\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "print('input:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('target:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): #batch dimention\n",
        "  for t in range(block_size): #time dimention\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THfhyctjtcHC",
        "outputId": "f0ec9c2a-74fa-4dce-f65b-f016480dd90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "target:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(xb) #our input to the tranformer (used to feed the neural network)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYdFysRhx-do",
        "outputId": "d3626c3e-5afd-4de7-931f-0e32a6363a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The simplest possible neural network that work good for LLM is the BigramLanguageModel\n",
        "#Let's implement a pytorch module that implements the  BigramLanguageModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module): #BigramLanguageModel is a subclass of NN module\n",
        "\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    #Creating a token embedding table that is of size vocab_size x vocab_size\n",
        "    #each token directly reads off logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "    #nn.Embedding is a very thin wrapper around a tensor of shape vocab_size x vocab_size\n",
        "\n",
        "  def forward(self, idx, targets=None): #target is optional becuse we use this function in situations were no loss is needed. By default is none\n",
        "    #idx and targets are both (B,T) tensor of integers\n",
        "    #when we pass idx to the token_embedding_table, every single integer in our input (idx) is\n",
        "    # going to refer to the embedding table and is going to pick up a row corrisponding to that index\n",
        "    # for example 24 will plcuk out the row 24 of the embedding table, and then 43, 58 etc\n",
        "    # pytorch is going to arrange all this in a batch x time x channels tensor. In this case\n",
        "    # is 4 x 8 x 65\n",
        "    # logits are basically the scores for the next character in the sequence\n",
        "    # we are predicting what's coming next based on echa individual token\n",
        "    logits = self.token_embedding_table(idx) #(B,T,C)\n",
        "\n",
        "    if targets is None: # in cases where we are interested just in the logits\n",
        "      loss = None\n",
        "    else:\n",
        "      #loss function\n",
        "      #A good way to measure a loss or like a quality of the poredictions\n",
        "      #is to use the negative log likelihood loss which is implemented\n",
        "      #implemented in pytorch under the name cross entropy\n",
        "      # however the cross entropy function (see documentation) expexts logits and targets in the\n",
        "      # form of (B*T,C) so we need to reshape our logits and targets\n",
        "      B, T, C = logits.shape #Returns the size of the self tensor\n",
        "      logits = logits.view(B*T, C) #Returns a new tensor with the same data as the self tensor but of a different shape\n",
        "      targets = targets.view(B*T) # or targets.view(-1) This is a way of telling the library:\n",
        "      #\"give me a tensor that has these many columns and you compute the appropriate number of rows that is necessary to make this happen\"\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "      # this measure the quality of the logits with respect to the targets\n",
        "      # we can guess the loss that we expect is -ln(1/vocab_size) to be guessing good\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B,T) array of indeces in the current\n",
        "    # this is the currenct context of some characters in some batch\n",
        "    # the goal of the generate function is to generate (B,T+1), (B,T+2) etc..\n",
        "    # so basically it continueus the generation in all the batch dimentions in the time dimention and it will do that for max_new_tokens\n",
        "    for _ in range(max_new_tokens):\n",
        "      #take the current indexes and get the predictions (logits) don't care about the loss\n",
        "      logits, loss = self(idx) #this end up going to the forwared function above\n",
        "      # focus only on the last time step (-1 is the last element in the time dimention)\n",
        "      logits = logits[:,-1,:] #becomes (B,C)\n",
        "      #apply softmax to get probabiloties\n",
        "      probs = F.softmax(logits,dim=-1) # (B,C)\n",
        "      #sample from the distribution of probabilities computed above and we ask pythorch to give us one sample\n",
        "      idx_next = torch.multinomial(probs,num_samples=1) # index_next is of type (B,1) becuse for each batch we are going to have a single new prediction\n",
        "      #append sampled index to the running sequence\n",
        "      # whatever character is predicted, it is concatenated with the given context in the time dimention\n",
        "      # that's become the next idx\n",
        "      idx = torch.cat((idx,idx_next), dim=1) # (B,T+1)\n",
        "    return idx\n",
        "\n",
        "#Building and calling the BigramLanguageModel\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "#creating a 1 by 1 tensor and it's holidng a zero and the data type is integer\n",
        "# so zero (new line) is were we are starting creating (the first character of the sequence we feed in)\n",
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "# starting generating from new line and asking for 100 tokens\n",
        "# taking row number zero from the generated batch which will contains a series\n",
        "# of indexes that will convert from a pytorch tensor into a python list so that we can feet it in the decode function to translate that into text\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "# we get garbage because the model is random and not trained\n",
        "# also to make a prediction we only look at the last character of the context and we don't take into account the history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd-QAQYIRytB",
        "outputId": "c8ef8a9d-7dcd-4d32-c21e-4409cbd38f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's train the model\n",
        "\n",
        "#create a PyTorch optimizer. We are using AdamW (SGD is the basic optimizer, but Adamw is better expecially for small NN )\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "mjZikELWTHUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets create the optimizer object that will take the gradients and updated the parameters using the gradients\n",
        "batch_size = 32\n",
        "\n",
        "#tyical training loop\n",
        "for step in range(10000):\n",
        "\n",
        "  #sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  #zeroising the gradients from the previous step\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  #getting the gradients for all the parameters\n",
        "  loss.backward()\n",
        "  #using those gradients to update our parameters\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8JPCWVeV60x",
        "outputId": "968e8f19-ae78-4f06-d9dc-b97cab504674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4552481174468994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "#This is a very basic model because the tokens are not talking to each other.\n",
        "#We are only looking at the very last character to make a prediction of what comes next\n",
        "#So now these tokens needs to talk to each other in order and figuring out what is in the context\n",
        "#so that they can make better prediction of what comes next and that's why we are going to use a TRANSFORMER!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y1k0QtEV-7u",
        "outputId": "fef130c9-cc8d-4707-f760-b4447beabf94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "F anfRERFoksighiaimfrfreranthangur-\n",
            "GOLLABAllile waricknthe y e pe s cke wbre cthe, thale las we, r,\n",
            "O t\n",
            "Tiea otry at aiey thivequrdrurencoue cy winchanot:\n",
            "GHEO:\n",
            "Butand, pe\n",
            "I ine anelaiolfrd ther.\n",
            "Angrthtr n t whe bofot lreaf hethilld brtou hance, ot beatous omyit gre orosathtis tt: th, rs ang my ourod;\n",
            "Thesst\n",
            "IOnd tingive she t, hang, WAUSO, hake ireys me s:\n",
            "ink'le. ay, outcathathafomeat, Yons angr hor s pexand, h puce s changot,\n",
            "MOMBu thage augd mou t o ngerksthe co s fr e XEYoouspeditheidrd's\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "38j7KXWAlN3Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
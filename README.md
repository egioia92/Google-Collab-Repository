# Google Collab Repository 
A collection of personal projects implemented in Google Collab
- NanoGPT: From Andrej Karpathy's class
- Reperti: Finetuning Stable Diffusion with ad hoc pictures

## NanoGPT
The base components of GPT come from a paper called "Attention is all you need" which proposed the Transformer architecture. 
GPT stands for Generatively Pretrained Transformer, so the transformer is the neural network that does all the heavy work under the wood.
The transformer architecture has been copy-pasted with small modifications in a huge number of applications in AI.
Nano GPT is a transformer-based language model in particular a character-level language model trained on all Shakespeare.

## Reperti
Exploring fine-tuning techniques for text-to-image models. Teaching Stable Diffusion with a new concept using images.
